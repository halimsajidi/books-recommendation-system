# -*- coding: utf-8 -*-
"""Books_recommendation_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lWtNPgnU1lctN3OQORzanGV7q5fTHrNa

# Proyek akhir: Books Recommendation System
- **Nama:** Halim Sajidi
- **Email:** halimsajidi14@gmail.com
- **ID Dicoding:** halimsajidi
- **Link Dataset:** https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data

## Persiapan

### Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### loading the dataset"""

books_df = pd.read_csv('Books.csv')
ratings_df = pd.read_csv('Ratings.csv')
users_df = pd.read_csv('Users.csv')

"""## Data Understanding

### Books Variabel
"""

books_df.info()

print('Banyak data: ', len(books_df['ISBN'].unique()))
print('Judul buku: ', books_df['Book-Title'].unique())
print('Penulis buku: ', books_df['Book-Author'].unique())
print('Tahun publikasi: ', books_df['Year-Of-Publication'].unique())
print('Publisher: ', books_df['Publisher'].unique())

"""Terdapat 82 ribu lebih data buku dan beberapa info terkait yang dapat dilihat pada hasil di atas.

#### Visualisasi
"""

# Menghitung jumlah buku per penulis
author_counts = books['Book-Author'].value_counts().head(10)

# Membuat bar plot penulis teratas
plt.figure(figsize=(10, 6))
sns.barplot(x=author_counts.values, y=author_counts.index, palette='viridis')
plt.title('Penulis Teratas Berdasarkan Jumlah Buku')
plt.xlabel('Jumlah Buku')
plt.ylabel('Penulis')
plt.show()

"""Berdasarkan gambar di atas, terdapat 10 author dengan jumlah buku terbanyak. walaupun sebenarnya masih belum bisa disimpulkan karena dari label jumlah buku nilai tertingginya di angka 10 ribu, dengan demikian simpulan awalnya adalah masih banyak data duplikat yang nantinya perlu di lakukan perbaikkan.

### Ratings Variabel
"""

ratings_df.info()

print('Banyak data: ', len(ratings_df['ISBN'].unique()))
print('Rating buku: ', ratings_df['Book-Rating'].unique())

"""Terdapat 32 ribu lebih data buku dan rating buku yang di nilai dengan rating antara 1 sampai 10

#### Visualisasi
"""

plt.figure(figsize=(10, 6))
sns.histplot(ratings_df['Book-Rating'], bins=10, kde=False, color='skyblue')
plt.title('Distribusi Peringkat Buku')
plt.xlabel('Peringkat Buku')
plt.ylabel('Jumlah Peringkat')
plt.xticks(range(11))  # Skala peringkat dari 0 hingga 10
plt.show()

"""Dapat dilihat berdasarkan grafik di atas, kebanyakkan user memberikan nilai rendah, banyak sekali user yang memberikan nilai 0 atau 1

### Users Variabel
"""

users_df.info()

print('Banyak data: ', len(users_df['User-ID'].unique()))
print('Lokasi user: ', users_df['Location'].unique())
print('Banyak Lokasi: ', len(users_df['Location'].unique()))
print('umur: ', users_df['Age'].unique())

"""Terdapat 27 ribu lebih data buku dengan lebih dari 57 ribu lokasi"""

users_df.describe()

"""Dapat dilihat kolom Age atau umur memiliki data yang tidak masuk akal, nilai minimum adalah 0 dan maksimumnya adalah 244

## Data Preprocessing

### Menggabungkan dataset books dengan ratings
"""

# Menggabungkan seluruh ISBN pada kategori Restaurant
books_all = np.concatenate((
    books_df.ISBN.unique(),
    ratings_df.ISBN.unique(),
))

# Mengurutkan data dan menghapus data yang sama
books_all = np.sort(np.unique(books_all))

print('Jumlah seluruh data buku berdasarkan ISBN: ', len(books_all))

books_all

"""### Menggabungkan dataset ratings dengan users"""

# Menggabungkan seluruh User-ID pada kategori Restaurant
users_all = np.concatenate((
    ratings_df['User-ID'].unique(),
    users_df['User-ID'].unique(),
))

# Mengurutkan data dan menghapus data yang sama
users_all = np.sort(np.unique(users_all))

print('Jumlah seluruh data buku berdasarkan User-ID: ', len(users_all))

"""### Menggabungkan file ratings_df dan books_df"""

# Menggabungkan file ratings_df, books_df ke dalam dataframe books_info
books_info = pd.merge(ratings_df, books_df, on='ISBN', how='left')
books_info

books_info.columns

books = books_info[['User-ID', 'ISBN', 'Book-Rating', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']]
books

"""Inilah data yang akan digunakan untuk membuat sistem rekomendasi.

## Data Preparation

### Menangani Missing Value
"""

books.isnull().sum()

"""Terdapat 418413 missing value. Langkah selanjutnya drop missing value, karena tidak bisa mengidentifikasikan nama buku yang tidak memiliki data."""

# Membersihkan missing value dengan fungsi dropna()
books_clean = books.dropna()
books_clean

"""Dapat dilihat dataset yang awalnya 1066941 rows × 7 columns menjadi 648528 rows × 7 columns setelah dilakukan drop nilai NaN"""

# Mengecek kembali missing value pada variabel all_resto_clean
books_clean.isnull().sum()

books_clean_sorted = books_clean.sort_values(by='Publisher')
books_clean_sorted

"""pada kolom Book-TItle dan Book-Author terdapat penulisan yang tidak sesuai seperti Ã?Â?ditions 10/18, maka diperlukan perlakuan lebih lanjut untuk masalah ini

Di sini diputuskan untuk mengubah karakter titik (.), koma(,), dan petik (') dengan string kosong.
"""

books_clean['Book-Title'] = books_clean['Book-Title'].str.replace(r"[.,']", '', regex=True)
books_clean['Publisher'] = books_clean['Publisher'].str.replace(r"[.,']", '', regex=True)

"""Digunakan regex untuk menemukan baris yang hanya mengandung huruf (a-z), angka (0-9)"""

books_clean = books_clean[books_clean['Book-Title'].str.match(r'^[a-zA-Z0-9]+$')]
books_clean = books_clean[books_clean['Publisher'].str.match(r'^[a-zA-Z0-9]+$')]

# Membuat variabel preparation yang berisi dataframe books_clean kemudian mengurutkan berdasarkan isbn
preparation = books_clean.copy()
preparation.sort_values('ISBN')

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation

"""### Mengkonversi data series menjadi list"""

# Mengonversi data series ‘ISBN’ menjadi dalam bentuk list
isbn = preparation['ISBN'].tolist()

# Mengonversi data series ‘Book-Title’ menjadi dalam bentuk list
book_title = preparation['Book-Title'].tolist()

# Mengonversi data series ‘Publisher’ menjadi dalam bentuk list
book_Publisher = preparation['Publisher'].tolist()

print(len(isbn))
print(len(book_title))
print(len(book_Publisher))

# Membuat dictionary untuk data isbn, book_title, dan book_author
book_new = pd.DataFrame({
    'id': isbn,
    'book_name': book_title,
    'publisher': book_Publisher
})
book_new

"""## Modeling and Result

### Content Based Filtering

#### TF-IDF Vectorizer

pada tahap ini dilakukan teknik TF-IDF yang akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap kategori product di amazon.
"""

data = book_new.copy()
data

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data publisher
tf.fit(data['publisher'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['publisher'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Untuk menghasilkan vektor tf-idf dalam bentuk matriks, kita menggunakan fungsi todense(). Jalankan kode berikut."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Selanjutnya, mari kita lihat matriks tf-idf untuk beberapa produk (Product_Name) dan kategori produk (Product_Category). Terapkan kode berikut."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.book_name
).sample(22, axis=1).sample(10, axis=0)

"""#### Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_name'], columns=data['book_name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""#### Mendapatkan Rekomendasi"""

def resto_recommendations(book_name, similarity_data=cosine_sim_df, items=data[['book_name', 'publisher']], k=5):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    book_name : tipe data string (str)
                Nama Restoran (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan resto sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_name agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

data[data.book_name.eq('AFTERMATH')]

"""Tabel di atas adalah user yang menyukai buku berjudul aftermath yang berasal dari publisher fireside."""

# Mendapatkan rekomendasi restoran yang mirip dengan KFC
resto_recommendations('AFTERMATH')

"""Berdasarkan hasil rekomendasi di atas, judul buku aftermah merupakan buku yang diterbitkan oleh publisher Fireside. Kemudian dari 5 rekomendasi yang diberikan, 4 dari 5 memiliki kesesuaian publisher

### Collaborative Filtering
"""

books_clean

"""#### Data Preparation

Pada tahap ini, dilakukan persiapan data untuk menyandikan (encode) fitur ‘User-ID’ dan ‘ISBN’ ke dalam indeks integer.
"""

# Mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = books_clean['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)

# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User-ID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User-ID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_ids = books_clean['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_ids)}

# Mapping userID ke dataframe user
books_clean['user'] = books_clean['User-ID'].map(user_to_user_encoded)

# Mapping placeID ke dataframe isbn
books_clean['isbn'] = books_clean['ISBN'].map(isbn_to_isbn_encoded)

books_clean.info()

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah isbn
num_isbn = len(isbn_encoded_to_isbn)
print(num_isbn)

# Mengubah Book-Rating menjadi nilai float
books_clean['Book-Rating'] = books_clean['Book-Rating'].values.astype(np.float32)

# Nilai minimum Book-Rating
min_Book_Rating = min(books_clean['Book-Rating'])

# Nilai maksimal Book-Rating
max_Book_Rating = max(books_clean['Book-Rating'])

print('Number of User: {}, Number of isbn: {}, Min Book-Rating: {}, Max Book-Rating: {}'.format(
    num_users, num_isbn, min_Book_Rating, max_Book_Rating
))

"""#### Membagi data untuk training dan validasi"""

books_clean.head()

# Mengacak dataset
books_clean = books_clean.sample(frac=1, random_state=42)
books_clean

# Membuat variabel x untuk mencocokkan data user dan isbn menjadi satu value
x = books_clean[['user', 'isbn']].values

# Membuat variabel y untuk membuat rating dari hasil
y = books_clean['Book-Rating'].apply(lambda x: (x - min_Book_Rating) / (max_Book_Rating - min_Book_Rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * books_clean.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""#### Proses training"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn = num_isbn
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_isbn,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_isbn, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2)

    x = dot_user_resto + user_bias + resto_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_isbn, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 20,
    validation_data = (x_val, y_val)
)

"""### Evaluation"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

book_clean = books_clean.copy()

# Membuang data duplikat pada variabel book_clean
book_clean = book_clean.drop_duplicates('ISBN')
book_clean

book_df = book_clean[['ISBN','Book-Title','Publisher']]
df = book_clean[['User-ID','ISBN','Book-Rating']]

book_df = book_df.rename(columns={
    'Book-Title': 'Title',
    'Book-Rating': 'Rating',
    'Publisher': 'Publisher'
})

df = df.rename(columns={
    'Book-Rating': 'Rating'
})

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
resto_visited_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
resto_not_visited = book_df[~book_df['ISBN'].isin(resto_visited_by_user.ISBN.values)]['ISBN']
resto_not_visited = list(
    set(resto_not_visited)
    .intersection(set(resto_to_resto_encoded.keys()))
)

resto_not_visited = [[resto_to_resto_encoded.get(x)] for x in resto_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_resto_array = np.hstack(
    ([[user_encoder]] * len(resto_not_visited), resto_not_visited)
)

ratings = model.predict(user_resto_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_resto_ids = [
    resto_encoded_to_resto.get(resto_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('books with high ratings from user')
print('----' * 8)

top_resto_user = (
    resto_visited_by_user.sort_values(
        by = 'Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

# Loop through top books visited by the user and print the title and publisher
book_df_rows = book_df[book_df['ISBN'].isin(top_resto_user)]
for row in book_df_rows.itertuples():
    print(row.Title, ':', row.Publisher)

print('----' * 8)
print('Top 10 books recommendation')
print('----' * 8)

# Loop through top recommended books and print the title and publisher
recommended_resto = book_df[book_df['ISBN'].isin(recommended_resto_ids)]
for row in recommended_resto.itertuples():
    print(row.Title, ':', row.Publisher)

"""hasil di atas adalah rekomendasi untuk user dengan id 105979. Dari output tersebut, kita dapat membandingkan antara books with high ratings from user dan Top 10 books recommendation untuk user."""

